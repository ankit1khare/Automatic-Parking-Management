{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Khare, Ankit\n",
    "# 1001-367-474\n",
    "# 2017-04-17\n",
    "# Project_final_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Identifying empty parking spaces in a Car Parking</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "A system is proposed which classifies the parking space as occupied or vacant. Such a system is highly useful in scenarios where a camera is placed at a lamp post view and parking spaces are visible. The system uses a combination of Laplacian operator for edge detection, HAAR classifier for object recognition and motion tracking to distinguish between the parked and vacant spaces. Motion tracking using background subtraction techniques, contours and Morphological operations is used. Further, Pedestrian detection using HOG and SVM is incorporated to detect pedestrains in the parking region.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupied and Vacant Parking Space Classification Approach\n",
    "HAAR classifier was compared with SVM and HOG. Although HOG combined with SVM gives more prevision in comparison to using HAAR alone but the processing required for HAAR is way lesser as compared to HOG. As a result, HAAR was finalized to be implemented. HOG's pedestrian detection inbuilt opencv2 function is used but it can be observed by turning it on by how how it educes the speed of execution. For enhancing the accuracy of classification as a vacant or occupied space, Laplacian operator is used. A threshold is established where the possibility of the presence of vehicle is maximum. Basically, parking regions are laid out manually and a threshold is defined by calculating the magnitude of edges inside of them. A car will have some significant edges, while empty parking spot will be smooth. This is the fact behind the use of Laplacian technique. If the possibility of the presence of a vehicle is beyond threshold then the classifier is applied to detect if it is a vehicle or not. Whenever a change in threshold is found, classifier is called to check whther it's a vehicle or not. It it is found to be a vehicle then it is marked and it is observed for motion. Whenever there's a motion at the point, again the classifier is called to determine the presence of vehicle. Hence the system proposes a combination of detection, tracking and classification to achieve efficient parking.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results\n",
    "A utility program was used to calculate the accuracy of the classifier in detecting the vehicles. The program applies classifier on a set of frames and calculate the percentage of vehicles identified in each frame. Next it displays the mean of the percentages. The results are as follows:\n",
    "\n",
    "* Classifier Trained with: **619 Positives and 1002 Negatives**\n",
    "* Number of Frames used: **5**\n",
    "* Mean of individual Accuracies: **17.841%**\n",
    "\n",
    "\n",
    "* Classifier Trained with: **1065 Positives and 1237 Negatives** (can be checked by right clicking 'haarTraining.bat' submitted file. Screenshots directory can be checked to see how training looks like.)\n",
    "* Number of Frames used: **5**\n",
    "* Mean of individual Accuracies:  **60.58%** (Please check the code for determining accuracy written in this notebook)\n",
    "\n",
    "\n",
    "Laplacian Threshold was changed and found to be most accurate with value 2.8 (Please check the attached threshold demonstration program)\n",
    "\n",
    "Threshold experimental result with total number of vehicles observed = 120\n",
    "\n",
    "* Threshold = 1.5  Vehicles misclassified = 54 \n",
    "\n",
    "* Threshold = 2.0  Vehicles misclassified = 39 \n",
    "\n",
    "* Threshold = 2.8  Vehicles misclassified = 19\n",
    "\n",
    "* Threshold = 3.0  Vehicles misclassified = 24\n",
    "\n",
    "* Threshold = 3.5  Vehicles misclassified = 38\n",
    "\n",
    "**Final Result with combination of threshold = 2.8 and classifier was 69% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Section\n",
    "1. Program to demonstrate the working of system\n",
    "2. Program to demonstrate classifier strength\n",
    "3. Program to demonstrate  the accuracy of classifier\n",
    "4. Program to show how to mark parking spaces in a video file and store them in yaml file to feed to the system for classification\n",
    "5. Program to get an estimate of Laplacian threshold for a video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Ankit Khare\n",
    "title: Smart Parking System\n",
    "\n",
    "Instructions: Escape key to termintae the program. Please press mutiple times if it doesn't work.\n",
    "Press u to jump 500 frames and J for 1000\n",
    "Values in the dictionary can be modified:-\n",
    "1. show_ids: turn id of parking areas on or off\n",
    "2. save_video: to save the video generated by program\n",
    "3. text_overlay: displaying the frame count at the left top corner\n",
    "4. motion_detection: turn on of off, motion detection\n",
    "5. pedestrian detection: slow due to use of opencv HOG inbuilt function\n",
    "6. min_area_motion_contour: min area to take for motion tracking\n",
    "7. start_frame: from which frame number to start\n",
    "8. park_laplacian_th: set threshold vsslues for different parkings\n",
    "\"\"\"\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# path references\n",
    "fn = \"Khare_testvideo_01.mp4\" #3\n",
    "#fn = \"datasets\\parkinglot_1_720p.mp4\"\n",
    "#fn = \"datasets\\street_high_360p.mp4\"\n",
    "fn_yaml = \"Khare_yml_01.yml\"\n",
    "fn_out =  \"Khare_outputvideo_01.avi\"\n",
    "cascade_src = 'Khare_classifier_02.xml'\n",
    "car_cascade = cv2.CascadeClassifier(cascade_src)\n",
    "global_str = \"Last change at: \"\n",
    "change_pos = 0.00\n",
    "dict =  {\n",
    "        'text_overlay': True,\n",
    "        'parking_overlay': True,\n",
    "        'parking_id_overlay': True,\n",
    "        'parking_detection': True,\n",
    "        'motion_detection': True,\n",
    "        'pedestrian_detection': False, # takes a lot of processing power\n",
    "        'min_area_motion_contour': 500, # area given to detect motion\n",
    "        'park_laplacian_th': 2.8, \n",
    "        'park_sec_to_wait': 1, # 4   wait time for changing the status of a region\n",
    "        'start_frame': 0, # begin frame from specific frame number \n",
    "        'show_ids': True, # shows id on each region\n",
    "        'classifier_used': True,\n",
    "        'save_video': False\n",
    "        }\n",
    "\n",
    "# Set from video\n",
    "cap = cv2.VideoCapture(fn)\n",
    "video_info = {  'fps':    cap.get(cv2.CAP_PROP_FPS),\n",
    "                'width':  int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)*0.6),\n",
    "                'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)*0.6),\n",
    "                'fourcc': cap.get(cv2.CAP_PROP_FOURCC),\n",
    "                'num_of_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, dict['start_frame']) # jump to frame number specified\n",
    "\n",
    "def run_classifier(img, id):\n",
    "    # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cars = car_cascade.detectMultiScale(img, 1.1, 1)\n",
    "    if cars == ():\n",
    "        return False\n",
    "    else:\n",
    "        # parking_status[id] = False\n",
    "        return True\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "if dict['save_video']:\n",
    "    fourcc = cv2.VideoWriter_fourcc('X','V','I','D') # options: ('P','I','M','1'), ('D','I','V','X'), ('M','J','P','G'), ('X','V','I','D')\n",
    "    out = cv2.VideoWriter(fn_out, -1, 25.0,(video_info['width'], video_info['height']))\n",
    "\n",
    "# initialize the HOG descriptor/person detector. Take a lot of processing power.\n",
    "if dict['pedestrian_detection']:\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "    # Use Background subtraction\n",
    "if dict['motion_detection']:\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(history=300, varThreshold=16, detectShadows=True)\n",
    "\n",
    "# Read YAML data (parking space polygons)\n",
    "with open(fn_yaml, 'r') as stream:\n",
    "    parking_data = yaml.load(stream)\n",
    "parking_contours = []\n",
    "parking_bounding_rects = []\n",
    "parking_mask = []\n",
    "parking_data_motion = []\n",
    "if parking_data != None:\n",
    "    for park in parking_data:\n",
    "        points = np.array(park['points'])\n",
    "        rect = cv2.boundingRect(points)\n",
    "        points_shifted = points.copy()\n",
    "        points_shifted[:,0] = points[:,0] - rect[0] # shift contour to region of interest\n",
    "        points_shifted[:,1] = points[:,1] - rect[1]\n",
    "        parking_contours.append(points)\n",
    "        parking_bounding_rects.append(rect)\n",
    "        mask = cv2.drawContours(np.zeros((rect[3], rect[2]), dtype=np.uint8), [points_shifted], contourIdx=-1,\n",
    "                                    color=255, thickness=-1, lineType=cv2.LINE_8)\n",
    "        mask = mask==255\n",
    "        parking_mask.append(mask)\n",
    "\n",
    "kernel_erode = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)) # morphological kernel\n",
    "kernel_dilate = cv2.getStructuringElement(cv2.MORPH_RECT,(5,19))\n",
    "if parking_data != None:\n",
    "    parking_status = [False]*len(parking_data)\n",
    "    parking_buffer = [None]*len(parking_data)\n",
    "# bw = ()\n",
    "def print_parkIDs(park, coor_points, frame_rev):\n",
    "    moments = cv2.moments(coor_points)\n",
    "    centroid = (int(moments['m10']/moments['m00'])-3, int(moments['m01']/moments['m00'])+3)\n",
    "    # putting numbers on marked regions\n",
    "    cv2.putText(frame_rev, str(park['id']), (centroid[0]+1, centroid[1]+1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame_rev, str(park['id']), (centroid[0]-1, centroid[1]-1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame_rev, str(park['id']), (centroid[0]+1, centroid[1]-1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame_rev, str(park['id']), (centroid[0]-1, centroid[1]+1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame_rev, str(park['id']), centroid, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "    \n",
    "while(cap.isOpened()):\n",
    "    video_cur_pos = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0 # Current position of the video file in seconds\n",
    "    video_cur_frame = cap.get(cv2.CAP_PROP_POS_FRAMES) # Index of the frame to be decoded/captured next\n",
    "    ret, frame_initial = cap.read()\n",
    "    if ret == True:\n",
    "        frame = cv2.resize(frame_initial, None, fx=0.6, fy=0.6)\n",
    "    if ret == False:\n",
    "        print(\"Video ended\")\n",
    "        break\n",
    "\n",
    "    # Background Subtraction\n",
    "    frame_blur = cv2.GaussianBlur(frame.copy(), (5,5), 3)\n",
    "    # frame_blur = frame_blur[150:1000, 100:1800]\n",
    "    frame_gray = cv2.cvtColor(frame_blur, cv2.COLOR_BGR2GRAY)\n",
    "    frame_out = frame.copy()\n",
    "\n",
    "    # Drawing the Overlay. Text overlay at the left corner of screen\n",
    "    if dict['text_overlay']:\n",
    "        str_on_frame = \"%d/%d\" % (video_cur_frame, video_info['num_of_frames'])\n",
    "        cv2.putText(frame_out, str_on_frame, (5,30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.8, (0,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame_out,global_str + str(round(change_pos,2)) + 'sec', (5, 60), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # motion detection for all objects\n",
    "    if dict['motion_detection']:\n",
    "        # frame_blur = frame_blur[380:420, 240:470]\n",
    "        # cv2.imshow('dss', frame_blur)\n",
    "        fgmask = fgbg.apply(frame_blur)\n",
    "        bw = np.uint8(fgmask==255)*255\n",
    "        bw = cv2.erode(bw, kernel_erode, iterations=1)\n",
    "        bw = cv2.dilate(bw, kernel_dilate, iterations=1)\n",
    "        # cv2.imshow('dss',bw)\n",
    "        # cv2.imwrite(\"frame%d.jpg\" % co, bw)\n",
    "        (_, cnts, _) = cv2.findContours(bw.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # print(cv2.contourArea(c))\n",
    "            # if the contour is too small, we ignore it\n",
    "            if cv2.contourArea(c) < dict['min_area_motion_contour']:\n",
    "                continue\n",
    "            (x, y, w, h) = cv2.boundingRect(c)\n",
    "            cv2.rectangle(frame_out, (x, y), (x + w, y + h), (255, 0, 0), 1)\n",
    "\n",
    "    # detecting cars and vacant spaces\n",
    "    if dict['parking_detection']:\n",
    "        for ind, park in enumerate(parking_data):\n",
    "            points = np.array(park['points'])\n",
    "            rect = parking_bounding_rects[ind]\n",
    "            roi_gray = frame_gray[rect[1]:(rect[1]+rect[3]), rect[0]:(rect[0]+rect[2])] # crop roi for faster calcluation\n",
    "\n",
    "            laplacian = cv2.Laplacian(roi_gray, cv2.CV_64F)\n",
    "            # cv2.imshow('oir', laplacian)\n",
    "            points[:,0] = points[:,0] - rect[0] # shift contour to roi\n",
    "            points[:,1] = points[:,1] - rect[1]\n",
    "            delta = np.mean(np.abs(laplacian * parking_mask[ind]))\n",
    "            # if(delta<2.5):\n",
    "                # print(\"ind, del\", ind, delta)\n",
    "            status = delta < dict['park_laplacian_th']\n",
    "            # If detected a change in parking status, save the current time\n",
    "            if status != parking_status[ind] and parking_buffer[ind]==None:\n",
    "                parking_buffer[ind] = video_cur_pos\n",
    "                change_pos = video_cur_pos\n",
    "                # print(\"state \", ind,delta)\n",
    "                # applying classifier in case a change is detected in the status of area\n",
    "                # if dict['classifier_used']:\n",
    "                #     classifier_result = run_classifier(roi_gray)\n",
    "                #     if classifier_result:\n",
    "                #         print(classifier_result)\n",
    "            # If status is still different than the one saved and counter is open\n",
    "            elif status != parking_status[ind] and parking_buffer[ind]!=None:\n",
    "                if video_cur_pos - parking_buffer[ind] > dict['park_sec_to_wait']:\n",
    "                    parking_status[ind] = status\n",
    "                    parking_buffer[ind] = None\n",
    "            # If status is still same and counter is open\n",
    "            elif status == parking_status[ind] and parking_buffer[ind]!=None:\n",
    "                parking_buffer[ind] = None\n",
    "\n",
    "    # changing the color on the basis on status change occured in the above section and putting numbers on areas\n",
    "    if dict['parking_overlay']:\n",
    "        for ind, park in enumerate(parking_data):\n",
    "            points = np.array(park['points'])\n",
    "            if parking_status[ind]:\n",
    "                color = (0,255,0)\n",
    "                rect = parking_bounding_rects[ind]\n",
    "                roi_gray_ov = frame_gray[rect[1]:(rect[1] + rect[3]),\n",
    "                               rect[0]:(rect[0] + rect[2])]  # crop roi for faster calcluation\n",
    "                res = run_classifier(roi_gray_ov, ind)\n",
    "                if res:\n",
    "                    parking_data_motion.append(parking_data[ind])\n",
    "                    # del parking_data[ind]\n",
    "                    color = (0,0,255)\n",
    "            else:\n",
    "                color = (0,0,255)\n",
    "            \n",
    "            cv2.drawContours(frame_out, [points], contourIdx=-1,\n",
    "                                 color=color, thickness=2, lineType=cv2.LINE_8)\n",
    "            if dict['show_ids']:\n",
    "                    print_parkIDs(park, points, frame_out)\n",
    "            \n",
    "            \n",
    "\n",
    "    if parking_data_motion != []:\n",
    "        for index, park_coord in enumerate(parking_data_motion):\n",
    "            points = np.array(park_coord['points'])\n",
    "            color = (0, 0, 255)\n",
    "            recta = parking_bounding_rects[ind]\n",
    "            roi_gray1 = frame_gray[recta[1]:(recta[1] + recta[3]),\n",
    "                            recta[0]:(recta[0] + recta[2])]  # crop roi for faster calcluation\n",
    "            # laplacian = cv2.Laplacian(roi_gray, cv2.CV_64F)\n",
    "            # delta2 = np.mean(np.abs(laplacian * parking_mask[ind]))\n",
    "            # state = delta2<1\n",
    "            # classifier_result = run_classifier(roi_gray1, index)\n",
    "            # cv2.imshow('dsd', roi_gray1)\n",
    "            fgbg1 = cv2.createBackgroundSubtractorMOG2(history=300, varThreshold=16, detectShadows=True)\n",
    "            roi_gray1_blur = cv2.GaussianBlur(roi_gray1.copy(), (5, 5), 3)\n",
    "            # cv2.imshow('sd', roi_gray1_blur)\n",
    "            fgmask1 = fgbg1.apply(roi_gray1_blur)\n",
    "            bw1 = np.uint8(fgmask1 == 255) * 255\n",
    "            bw1 = cv2.erode(bw1, kernel_erode, iterations=1)\n",
    "            bw1 = cv2.dilate(bw1, kernel_dilate, iterations=1)\n",
    "            # cv2.imshow('sd', bw1)\n",
    "            # cv2.imwrite(\"frame%d.jpg\" % co, bw)\n",
    "            (_, cnts1, _) = cv2.findContours(bw1.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            # loop over the contours\n",
    "            for c in cnts1:\n",
    "                print(cv2.contourArea(c))\n",
    "                # if the contour is too small, we ignore it\n",
    "                if cv2.contourArea(c) < 4:\n",
    "                    continue\n",
    "                (x, y, w, h) = cv2.boundingRect(c)\n",
    "                classifier_result1 = run_classifier(roi_gray1, index)\n",
    "                if classifier_result1:\n",
    "                # print(classifier_result)\n",
    "                    color = (0, 0, 255)  # Red again if car found by classifier\n",
    "                else:\n",
    "                    color = (0,255, 0)\n",
    "            classifier_result1 = run_classifier(roi_gray1, index)\n",
    "            if classifier_result1:\n",
    "                # print(classifier_result)\n",
    "                color = (0, 0, 255)  # Red again if car found by classifier\n",
    "            else:\n",
    "                color = (0, 255, 0)\n",
    "            cv2.drawContours(frame_out, [points], contourIdx=-1,\n",
    "                                 color=color, thickness=2, lineType=cv2.LINE_8)\n",
    "\n",
    "    if dict['pedestrian_detection']:\n",
    "        # detect people in the image. Slows down the program, requires high GPU speed\n",
    "        (rects, weights) = hog.detectMultiScale(frame, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "        # draw the  bounding boxes\n",
    "        for (x, y, w, h) in rects:\n",
    "            cv2.rectangle(frame_out, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    # write the output frames\n",
    "    if dict['save_video']:\n",
    "#         if video_cur_frame % 35 == 0: # take every 30 frames\n",
    "            out.write(frame_out)\n",
    "\n",
    "    # Display video\n",
    "    cv2.imshow('frame', frame_out)\n",
    "    # cv2.imshow('background mask', bw)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "    elif k == ord('c'):\n",
    "        cv2.imwrite('frame%d.jpg' % video_cur_frame, frame_out)\n",
    "    elif k == ord('j'):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, video_cur_frame+1000) # jump 1000 frames\n",
    "    elif k == ord('u'):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, video_cur_frame + 500)  # jump 500 frames\n",
    "    if cv2.waitKey(33) == 27:\n",
    "        break\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cap.release()\n",
    "if dict['save_video']: out.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution finished\n",
      "Execution finished\n",
      "Execution finished\n",
      "Execution finished\n"
     ]
    }
   ],
   "source": [
    "# Classifier demonstration on different videos:\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "def perform_classification(video_src, cascade_src):\n",
    "    cap = cv2.VideoCapture(video_src)\n",
    "    car_cascade = cv2.CascadeClassifier(cascade_src)\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if (type(img) == type(None)):\n",
    "            print('Video not found')\n",
    "            break\n",
    "        image_scaled = cv2.resize(img, None, fx=0.6, fy=0.6)\n",
    "        gray = cv2.cvtColor(image_scaled, cv2.COLOR_BGR2GRAY)\n",
    "        cars = car_cascade.detectMultiScale(gray, 1.1, 1) #1.1, 1\n",
    "        cars = np.array([[x, y, x + w, y + h] for (x, y, w, h) in cars])\n",
    "        pick = non_max_suppression(cars, probs=None, overlapThresh=0.65)\n",
    "        for (x, y, w, h) in pick:\n",
    "            # cv2.rectangle(image_scaled, (x, y), (x + w, y + h), (0 , 255, 255), 2) #bgr\n",
    "            cv2.rectangle(image_scaled, (x, y), (w,  h), (0, 255, 255), 2)\n",
    "        cv2.imshow('Press ESC key to finish', image_scaled)\n",
    "\n",
    "            # press escape key to exit\n",
    "        if cv2.waitKey(33) == 27:\n",
    "            break\n",
    "    print('Execution finished')\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "perform_classification('Khare_testvideo_01.mp4', 'Khare_classifier_01.xml')    # press escape to finish\n",
    "perform_classification('Khare_testvideo_01.mp4', 'Khare_classifier_02.xml')\n",
    "perform_classification('Khare_testvideo_02.avi', 'Khare_classifier_01.xml')    # M6 highway Britain\n",
    "perform_classification('Khare_testvideo_02.avi', 'Khare_classifier_02.xml')    # M6 highway Britain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 vehicles found\n",
      "28 vehicles found\n",
      "23 vehicles found\n",
      "26 vehicles found\n",
      "34 vehicles found\n",
      "Accuracy after evaluating 5 frames and assuming correct identification:  17.840938254950995\n",
      "86 vehicles found\n",
      "97 vehicles found\n",
      "105 vehicles found\n",
      "84 vehicles found\n",
      "100 vehicles found\n",
      "Accuracy after evaluating 5 frames and assuming correct identification:  60.58245363977848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.58245363977848"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measuring accuracy of classifier. Frames are taken in different light conditions (morning, afternoon and evening)\n",
    "import Khare_utility_01 as util\n",
    "\n",
    "util.get_acc('Khare_classifier_01.xml')\n",
    "util.get_acc('Khare_classifier_02.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# press escape to finish doing real time boxing.\n",
    "# Program marks the polygons in the figure when it gets 4 double clicks\n",
    "import cv2\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "refPt = []\n",
    "cropping = False\n",
    "data = []\n",
    "file_path = 'Khare_yml_02.yml'\n",
    "img = cv2.imread('Khare_frame_02.png')\n",
    "\n",
    "def yaml_loader(file_path):\n",
    "    with open(file_path, \"r\") as file_descr:\n",
    "        data = yaml.load(file_descr)\n",
    "        return data\n",
    "\n",
    "\n",
    "def yaml_dump(file_path, data):\n",
    "    with open(file_path, \"a\") as file_descr:\n",
    "        yaml.dump(data, file_descr)\n",
    "\n",
    "\n",
    "def yaml_dump_write(file_path, data):\n",
    "    with open(file_path, \"w\") as file_descr:\n",
    "        yaml.dump(data, file_descr)\n",
    "\n",
    "\n",
    "def click_and_crop(event, x, y, flags, param):\n",
    "    current_pt = {'id': 0, 'points': []}\n",
    "    # grab references to the global variables\n",
    "    global refPt, cropping\n",
    "    if event == cv2.EVENT_LBUTTONDBLCLK:\n",
    "        refPt.append((x, y))\n",
    "        cropping = False\n",
    "    if len(refPt) == 4:\n",
    "        if data == []:\n",
    "            if yaml_loader(file_path) != None:\n",
    "                data_already = len(yaml_loader(file_path))\n",
    "            else:\n",
    "                data_already = 0\n",
    "        else:\n",
    "            if yaml_loader(file_path) != None:\n",
    "                data_already = len(data) + len(yaml_loader(file_path))\n",
    "            else:\n",
    "                data_already = len(data) \n",
    "        \n",
    "        cv2.line(image, refPt[0], refPt[1], (0, 255, 0), 1)\n",
    "        cv2.line(image, refPt[1], refPt[2], (0, 255, 0), 1)\n",
    "        cv2.line(image, refPt[2], refPt[3], (0, 255, 0), 1)\n",
    "        cv2.line(image, refPt[3], refPt[0], (0, 255, 0), 1)\n",
    "\n",
    "        temp_lst1 = list(refPt[2])\n",
    "        temp_lst2 = list(refPt[3])\n",
    "        temp_lst3 = list(refPt[0])\n",
    "        temp_lst4 = list(refPt[1])\n",
    "\n",
    "        current_pt['points'] = [temp_lst1, temp_lst2, temp_lst3, temp_lst4]\n",
    "        current_pt['id'] = data_already\n",
    "        data.append(current_pt)\n",
    "        # data_already+=1\n",
    "        refPt = []\n",
    "image = cv2.resize(img, None, fx=0.6, fy=0.6)\n",
    "clone = image.copy()\n",
    "cv2.namedWindow(\"Double click to mark points\")\n",
    "cv2.imshow(\"Double click to mark points\", image)\n",
    "cv2.setMouseCallback(\"Double click to mark points\", click_and_crop)\n",
    "\n",
    "# keep looping until the 'q' key is pressed\n",
    "while True:\n",
    "    # display the image and wait for a keypress\n",
    "    cv2.imshow(\"Double click to mark points\", image)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if cv2.waitKey(33) == 27:\n",
    "        break\n",
    "       \n",
    "# data list into yaml file\n",
    "if data != []:\n",
    "    yaml_dump(file_path, data)\n",
    "cv2.destroyAllWindows() #important to prevent window from becoming inresponsive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  2.47696721429\n",
      "median:  2.37567587029\n"
     ]
    }
   ],
   "source": [
    "# Program to demonstrate the calculation of threshold value for a given parking arrangement\n",
    "import statistics\n",
    "import cv2\n",
    "import yaml\n",
    "import numpy as np\n",
    "        \n",
    "sum_up = 0.0\n",
    "delta_list = []\n",
    "frame = cv2.imread('Khare_frame_02.png') #\n",
    "parking_bounding_rects = []\n",
    "parking_mask = []\n",
    "frame_blur = cv2.GaussianBlur(frame.copy(), (5,5), 3)\n",
    "frame_gray = cv2.cvtColor(frame_blur, cv2.COLOR_BGR2GRAY)\n",
    "with open('Khare_yml_03.yml', 'r') as stream:\n",
    "    parking_data = yaml.load(stream)\n",
    "    \n",
    "if parking_data != None:\n",
    "    for park in parking_data:\n",
    "        points = np.array(park['points'])\n",
    "        rect = cv2.boundingRect(points)\n",
    "        points_shifted = points.copy()\n",
    "        points_shifted[:,0] = points[:,0] - rect[0] # shift contour to region of interest\n",
    "        points_shifted[:,1] = points[:,1] - rect[1]\n",
    "        \n",
    "        parking_bounding_rects.append(rect)\n",
    "        mask = cv2.drawContours(np.zeros((rect[3], rect[2]), dtype=np.uint8), [points_shifted], contourIdx=-1,\n",
    "                                    color=255, thickness=-1, lineType=cv2.LINE_8)\n",
    "        mask = mask==255\n",
    "        parking_mask.append(mask)\n",
    "\n",
    "for ind, park in enumerate(parking_data):\n",
    "        points = np.array(park['points'])\n",
    "        rect = parking_bounding_rects[ind]\n",
    "        roi_gray = frame_gray[rect[1]:(rect[1]+rect[3]), rect[0]:(rect[0]+rect[2])] # crop roi for faster calcluation\n",
    "\n",
    "        laplacian = cv2.Laplacian(roi_gray, cv2.CV_64F)\n",
    "        points[:,0] = points[:,0] - rect[0] # shift contour to roi\n",
    "        points[:,1] = points[:,1] - rect[1]\n",
    "        delta = np.mean(np.abs(laplacian * parking_mask[ind]))\n",
    "        if(delta > 1.8):    # ignoring empty spaces\n",
    "            delta_list.append(delta)\n",
    "            # print(delta)\n",
    "        sum_up = sum_up + delta\n",
    "        \n",
    "avg = sum_up/len(parking_data)\n",
    "med = statistics.median(delta_list)\n",
    "print(\"mean: \", avg)\n",
    "print(\"median: \", med)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The system proposes an effective methodology to manage parking system using a camera placed for a lamp-post view. The accuracy of the system is dependent on the laplacian threshold, motion tracking algorithm based on background subtraction and most importantly, the strength of the classifier. The challenge faced was to determine motion in a small region of interest when there's a high possibility of noise. For example, the camera was placed near the stairs and the reflection of people walking down the stairs was being reflected in the input video provided by camera. It becomes challenging to reduce the impact of such noises and detect the small amount of motion. Besides, HAAR classifier does a decent job but implementing a classifier based on Convolution Neural Networks will greatly increase the overall accuracy of the system. This is the future scope of work in this project. For now, the system has been found working with an accuracy of more than 68% with 4 parking lots.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### 1. Video Tutorials \n",
    "-https://www.youtube.com/watch?v=bPeGC8-PQJg\n",
    "\n",
    "-https://www.youtube.com/watch?v=Dg-4MoABv4I\n",
    "\n",
    "\n",
    "### 2. Codes\n",
    "-https://github.com/eladj/detectParking\n",
    "\n",
    "-http://www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv/\n",
    "\n",
    "### 3. Papers\n",
    "[1] M. I. et al, “Car park system: A review of smart parking system and its technology,” Information Technology Journal, 2009.\n",
    "In this study, various types of smart parking systems with their pros and cons are presented.\n",
    "\n",
    "[2] N. True, “Vacant parking space detection in static images,” 2007. http://cseweb.ucsd.edu/classes/wi07/cse190-a/reports/ntrue.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
